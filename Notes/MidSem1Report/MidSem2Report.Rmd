---
title: "Code-based open-source software for teaching interactive data visualisation"
subtitle: "Mid Semester 2 Report"
author: "Shan-I Lee"
date: "8/26/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) # For tidying of NCEA data
```

## Introduction
This report will discuss the developments since last semester on using code-based open-source software for teaching interactive data visualisation. The insights gained from developing an exemplar of interactive techniques "in action", will form the foundation of discussion. Changes in the focus of research will be identified, followed by a consolidation of "new" interactive software with those previously examined. 

### Exemplar of interactive techniques
A narrative on how ideas for the exemplar emerged and change over its development will be outlined and discussed with respect to the insights gained on applying interactive techniques. 
The motivation to apply interactive visualisation to cluster analysis emerged from reading literature on the topic, as well as finding "real" data for which it was of interest to see if there was any such underlying structure. Data on the performance of New Zealand schools in the National Certificate of Educational Achievement (NCEA), at the four qualification levels, Level One, Two, Three and University Entrance, can be obtained from the New Zealand Qualifications Authority (NZQA) website. Information on the decile, region and a "small" cohort warning, were also included in the data. The achievement rate of a school for each qualification level was quantified in a few ways. The achievement indicator chosen for analysis was the proportion of students at the school who were successful in obtaining the qualification level, given that they were entered in enough standards to have an opportunity to earn the qualification in the 2016 school year. This is referred to as the "Current Year Achievement Rate" for the "Participating Cohort" by the NZQA (see <http://www.nzqa.govt.nz/assets/Studying-in-NZ/Secondary-school-and-NCEA/stats-reports/NZQA-Secondary-Statistics-Consolidated-Data-Files-Short-Guide.pdf>). 
Only Auckland schools with achievement indicators across all four qualification levels were retained in the analysis to minimise the unreliability of rates from small samples sizes (i.e. small schools). The NZQA indicator of a "small" cohort was at a very low threshold of fewer than five candidates for a qualification level. Being the most populated city in New Zealand, Auckland has many of the larger schools, but there may still be some schools left in the analysis that have less than 30 candidates entered in a qualification level. The first few observations from the data set are shown below.

```{r, echo=FALSE}
nzqa2016 <- read.csv("http://www.nzqa.govt.nz/assets/Studying-in-NZ/Secondary-school-and-NCEA/stats-reports/2016/Qualification-Statistics-School-2016-29032017.csv")
# Drop vars that will not be used (eg. cumulative achievement)
nzqa <- nzqa2016[,-c(1,8,10)]
names(nzqa) <- c("Decile", "Region", "School", "Year", "Qualification", "Achieve_participate", "Achieve_roll", "Small_sch")
levels(nzqa$Qualification) <- c("L1", "L2", "L3", "UE")
# Subset to use only Year 11 with Level 1, etc
nzqa <- nzqa %>% filter(((Qualification=="L1") & (Year==11)) | 
                          ((Year==12) & (Qualification=="L2")) |
                          ((Year==13) & (Qualification=="L3")) | 
                          ((Year==13) & (Qualification=="UE"))) %>%
  # Reshape so that each row represents an observation (a school)
  # Current.Achievement.Rate.Participation kept for analysis only
  spread(Qualification, Achieve_participate, fill=0) %>%
  group_by(School) %>%
  summarise_at(c("L1", "L2", "L3", "UE"), sum) %>%
  inner_join(nzqa[, c(1, 2, 3, 8)]) %>% # Add Decile and Region and Small_sch variables
  distinct() %>% 
  filter(!((L1==0)&(L2==0)&(L3==0))) #Remove schools with 0% achievement rate for all levels

# Function to replace 0% with NA
zeros <- function(col) {
  replace(col, col==0, NA)
}

nzqa[2:5] <- sapply(nzqa[2:5], zeros)
# Remove schools with 'small cohort' warning (obscures pattern in non-small cohorts).
nzqa <- nzqa[nzqa$Small_sch=="",] # 437 school left
nzqa <- nzqa[, -8] # Remove Small_sch var
# Remove schools with any NA values 
nzqa <- nzqa[complete.cases(nzqa),] # 407 schools left
nzqa$Decile <- as.factor(nzqa$Decile)
# Subset Auckland schools only
akl <- as.data.frame(nzqa[nzqa$Region=="Auckland", -7]) #90 schools in Akl
rownames(akl) <- akl$School
akl <- akl[-1] # Delete "School" var
head(akl, n=3)
```


* Process of finding NCEA data and then question prompted classfication analysis (static tree) - show. Unable to identify and brush. Black box, data in model space representation?
Developed single tour first.
Central to the exemplar is the 2D tour. 
* Reasons for using a tour:
It was proposed in the previous report that missing values and cluster analysis would be motivating contexts for students to apply interactive visualisation techniques to.
Demostrations involving tours often use pre-recordings of tours. Although the recordings are dynamic, the level of interactivity is limited to rewinding. The stochastic nature of tours begs for more user interaction than as a post-event observer. 
Being able to interact with several "live" tours that still show the same signals, provides the objectivity and rigour that some may criticise as lacking from GDA.
Stochastic processes: visuals are apt for contextualising the signal in the context of the variation inherent in random processes (see blindfold).
Numerical methods: Visuals allow us to "get under the hood" of black-box numerical methods to gain a better understanding of the process and the solutions produced (blindfold). Visual representations provide a way to make large volumes of numerical output meaningful, with or without summarising, so that patterns and connections can be identified across several iterations. Applying interactive techniques leverages the visuals to bring together even more information and enable deeper insights.
This exemplar also exploits the use of "law of small multiples" (see Seivert and trelliscope). Blindfold advocates for many views together rather than one alone.
* Cluster analysis capitalises on some of the strengths of visual representations.
It makes sense to support answers to the following questions with visual evidence, rather than numerical measures alone:
- How many clusters are there? How big are the clusters?
- What shape are the clusters? Do they overlap?
Coloring plots according to cluster membership identified by numerical methods helps with the interpretation and evaluation of the model (Cook p126). When groups are clearly separated, a graphical approach may be sufficient alone, without the needs to satisfy assumptions about equality of variance or linear boundaries.
How does interactive techniques add more?
* Tooltip identification allows you to quickly query outliers and unusual values, then being able to brush and link to other variables to identify features of these outliers. eg. Auckland Grammar, without contextual knowledge of Auckland schools we may not think it is unusual for Grammar to be in the lower achieving cluster but by linking the plot of the tour projection to a plot showing the decile rating of the school we can see why it's unusual. (Grammar not necessarily that unusual in its values for L1 to L3 and UE, when compared to all schools but combined with the data on its decile rating, it is unusual. Cannot pick this up without linked brushing. Also the tour view allows us to identify this, it would have been difficult to single out Grammar in the pairs plots. No clear separation into clusters in the univariate or bivariate distributions)
* Linked brushing of plots can help to answer the questions that we are interested in when evaluating clusters (me):
- Which variables contribute the most to the clustering? 
- Can the clusters be qualitatively described?
Linking tour and pairs plot (addition of visual info about loadings of PCAs). Intuitively there is an underlying desire to view the model in data space (blindfold)
Cluster analysis is appropriate only as an explorative technique, but it is a powerful tool for data simplification (Cook 107). Cluster analysis 'tends to produce hypotheses rather than testing them' (p.107).
NCEA data: initially started with the intention of using a numerical method to model clusters and then using visualisation to interpret and analyse the model (ie. using interactive techniques after analysis) but evolved into developing an exploratory tool for identifying clusters instead.
To do: How easy is it to modify current app so that it can be used for original purpose? (evaluation after numerical analysis). Replace index plot with heirachical cluster model (ie. how the clusters are being found has been changed) and...

### Shifts in focus 
Shift away from large data sets: Troubleshooting can be difficult for novices and errors are costly in terms of time.
"New" datasets: Mixture of something "tried and true" (olives was suggested last time), crabs data used. Did we reveal something new? Species split is more obvious than gender, Orange species easier to differentiate than Blue (3 groupings appear sometimes) - Check if either Cook or Mass note these.
Choosing own dataset to try techniques/tools on was motivating, but good to test out techniques and tools on tried and true examples (Using crabs dataset prompted further investigation into how the tourr package chose the starting point for tours since some of the views described in the literature were not being observed. The choice of starting point is very important in tours since a stochastic process is used to search the neighbourhood for a view that is more "interesting" than any previously seen. If after a certain number of attempts no such view is found then the tour ends. So a tour can get "stuck" at a local maxima, hence the need to examine multiple starting points. A random tour eventually covers the whole data space but it can be challenging to retain so many views for comparison and know when to stop the tour in order to avoid being similarly trapped in a relative comparison.
Hence the idea of "maximising" the tour coverage by launching 2D tours from orthogonal projections of different variable pairs and collating their outcomes through interactive visuals - for comparison between tours and analysis within each tour. The projection pursuit index plot not only compares the index values reached by the tours, but also gives insight into the iterative process, the "length" of tours. Linking this plot to an animation of the tour allows direct access to the views associated with certain levels of the index. In the crabs data we often see two groupings in index levels when using the "holes" projectio pursuit. By inspecting the tours related to these projection we can see 

### Interactive techniques and software
* rggobi/ggobi: Showcase dynamic, interactive visualisation techniques at its best. rggobi a way to document and replicate visual explorations using code (a bridge into ggobi). The challenge of retaining views when observing tours in ggobi and being able to inspect previous projections in more detail, led to the motivation to find a way to document, review and compare tours. After working with ggobi there was a desire to document the interactions so that it could be easily reproduced and shared (and can pick up where left off). Capture an useful view.

* trelliscopejs, Alteryx and Tableau? Discuss? More custom made, trying to balance general and provide guidance on when to apply. But restrictive in user modification, creating something bespoke (discussed later)

* Definite-must-teach techniques: motivation to learn outweighs the learning curve required. Motivation to learn determined by how useful the techniques are perceived to be, through exposure to its use in a range of ways (?) and having to use it yourself to achieve something meaningful. There is a "demand" for quick but reliable analytics that can be delivered by well chosen visualisations with interactive techniques applied to allow for further analysis and understanding. Apply interactive techniques could also aid with explaining complex concepts to non-statisticians. 
Through exploring the use of interactive techniques with tours I have been able to troubleshoot misunderstandings of underlying processes and test conjectures to explore abstract concepts like principal components.  
Linked brushing enables the plots to become a whole that is greater than the sum of its parts. (Linked brushing exemplifies Aristotle's quote, "The whole is greater than the sum of its parts"). Tooltips for identification become easily taken for granted once one becomes accustomed to this basic interactive technique. This is definitely one technique that is favourable in all aspects (ease of application, widely applicable, hence motivation to learn outweighs the learning curve required, greatly). Both linked brushing and tooltip identification were used in the exemplar, in addition to guided tour visualisations. Tours are more abstract and hence may seem less applicable since the results are not directly linked to the "raw data". However tours provide one of the few ways for us to visualise multidimensional space and hence allows inspection of joint distributions and models in data space (blindfold).
Linked brushing provides a way to visualise the model in data space? (brushed points in tour projection linked with 2D pairs plot in data space - but low-D data space). Added value to other interactive visualisation techniques, ie. tours, relating model back to the data space.
Interactive techniques that may not be as general - deleting, changing points on a plot (Alteryx allows points to be dragged in the `Network Analysis` tool, but not in the other dashboards. In comparison deletion, changing values in any scatterplot and parallel coordinate plot in ggobi is possible. Onus is on the user to decide what techniques are appropriate for their dataset). Tours:

### Insights from developing an exemplar
Recent developments in open-source interactive software (i.e. shiny, crosstalk, plotly) allow for tools to be be shared and reproduced easily via code documentation. In contrast explorations carried out in ggobi or Mondrian are typically shared through a screencast with commentary and hence cannot be "instantaneously" reproduced. The code documentation also allows for re-use and modification to analyse another dataset.
With this in mind users are able to either produce bespoke or general interactive applications.
* Bespoke creations vs general interactive tools (flows on from must-have-techniques).
Apply a collection of interactive techniques to build an "app" for a specific dataset (eg. NCEA data) or a set of similar datasets (eg. apply to data where identifying clusters is of interest). On the latter extreme of this spectrum is building a package as opposed to writing code for a specific dataset (see code for exemplar above). The coding skills required increases as the app becomes more general. Hence allowing students to choose where on the spectrum they wish to work helps to cater for a range of programming expertise and levels the playing field for assessment on the quality of visuals and interactive techniques applied. The exemplar that was developed sits in between the two extremes. 
The desire to add more and more interactive details tends to pull an app towards the bespoke end, while the desire keep things general to maximise the return from amount of time and effort invested in creating the app. Regardless of where on the spectrum an interactive visualisation app sits, students will be able to draw on this experience of applying interactive visualisation techniques, in future projects. Furthermore since the software used are open-source students can reuse or update chunks of code to quickly demonstrate how interactive visuals can add value to analysis.