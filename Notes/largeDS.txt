Large data notes:

+ Don’t need to plot all the data.
 - Summary (proportions, averages)
 - Binning techniques, R Base: smoothScatter(), statbinhex().
 - Density plots 
 - Alpha blending: Give each point a weight between 0 and 1 (1=no alpha blending). So that when points overlap the area is darker (more opaque). Eg. if alpha=0.1 then the area will be at maximum darkness when there are 10 points at the same location. Interactive: Explore a range of alpha values to see the diff patterns identified at diff levels (slider for alpha). (Unwind, 2007, p.93).

+ Exporting and viewing graphics:	
 - Vector based formats: (eg. PDF, SVG, XML, Javascript)
  + Takes a long time to create as well as to open later, but a sharp image.
  + Large files since info for each object is retained (object-orientated).
  + ggobi and ggvis are faster at updating images than Shiny since no ‘middleman’ involved (and they interact directly with the plot?)
 - Raster formats: (eg. PNG, JPEG)
  + Fixed number of pixels, so the file will remain small regardless of complexity of plot.
  + Fast to view plot but no information on objects stored (only interested in the ‘shading’/image of each pixel).
  + Exporting as a PNG in R is relatively faster than PDF, but can still take some time when the graph is complex.  Once stored as PNG, image is fast to view.
  + Images are grainy when enlarged.

+ Genevera Allen (22 March 2017 Ihaka Series)
https://www.stat.auckland.ac.nz/en/about/news-and-events-5/events/events-2017/03/ihaka-genevera-allen.html
 - Clustering (and bi-clustering) are examples of exploratory data analysis. Uses dendrograms and k-means visuals.
 - Interactive Clustering Visualisation. Goal: Build tools to be able to interactively and dynamically see how the data groups together, “visually see how elements of the data are linked together” via dendrograms.  Visualise the solution path.
 - They had to modify the algorithm so that it found a reliable approximate clustering path in order for it to be realistic to visualise the process in real time. (The existing algorithm took over a week with only 50 data points).
 ? Maybe similar “approximating” modifications may need to be made to the way we visualise data in order for it to be extended to large data sets.

+ Ross Ihaka’s talk (29 March 2017 Ihaka Series)
 - Making R more ‘static’ and less dynamic, so that it can be more efficient and hence reduce run-time.  A more ‘static’ R will allow better use with large data sets.  Also allow advances in on-plot interactions within R?
 - Need for statisticians to be involved in the building of tools and not to just to have programmers do it.  My thoughts on implications for interactive tools: there is general audience where the interaction is more for presentation, animation and engagement, as opposed to the purposes of exploratory analysis, model and solution checking required for statisticians. There is overlap but maybe recent developments in interactive tools have been more focused on the former rather than the latter?

Course on Web Scraping using R:
http://blog.rolffredheim.com/

